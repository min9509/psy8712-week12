---
title: "PSY 8712 Week 12 Project"
author: "Minjeong Seo"
date: "`r Sys.Date()`"
output: 
  html_document:
    df_print: paged
---

# **Script Settings and Resources**
This chunk is for script setting and loading packages.
```{r setup, message=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tidytext)
library(RedditExtractoR)
library(tm)
library(textstem)
library(qdap)
library(ldatuning)
library(RWeka)
library(topicmodels)
library(parallel)
library(doParallel)
library(wordcloud)
```

# **Data Import and Cleaning**
This chunk is responsible for importing and cleaning data from Reddit. 
```{r data import, message=FALSE}
# reddit_url <- find_thread_urls(
#  subreddit = "IOPsychology",
#  period = "year")

## Getting content of threads using retrieved URLs
# reddit_data <- get_thread_content(reddit_url$url)

## Extracting titles and upvotes 
# title <- reddit_data$thread$title
# upvotes <- reddit_data$thread$upvotes

## Creating a tibble from the extracted data
# week12_tbl <- tibble(title, upvotes)

## Writing the tibble to a CSV file in the "data" directory
# write_csv(week12_tbl, "../data/week12_tbl.csv")

# Loading the data from the saved CSV file into a new variable
week12_tbl <- read_csv("../data/week12_tbl.csv")
```

This chunk is for text formation. 
```{r corpus, message=FALSE}
# Importing the original corpus from week12_tbl$title
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

# Preprocessing the original corpus using various text transformations
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>%
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("en")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_words))

# Remove the zero context
io_corpus <- io_corpus %>%
  tm_filter(FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 0) })


# Function to compare randomly selected rows from two corpus
compare_them <- function(corpus1, corpus2) {
  index <- sample(1:length(corpus1),1)
  origianl <- print(corpus1[[index]]$content)
  new <- print(corpus2[[index]]$content)
}

# Example usage to compare preprocessed and original corpora
compare_them(io_corpus_original, io_corpus)
```

This chunk is for creating DTM and slim DTM.
```{r dtm, message=FALSE}
# Create a custom tokenizer function using NGramTokenizer with n-gram range 1:2
myTokenizer <- function(x) {NGramTokenizer(x, Weka_control(min=1, max=2))}

# Create a DTM using the custom tokenizer function
io_dtm_zero <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
zero_count <- apply(io_dtm_zero, 1, sum)
io_dtm <- io_dtm_zero[zero_count > 0, ]

# Create a version of the DTM with sparse terms eliminated
# The sparse terms are those that appear in less than 0.3% (1 - 0.997) of the documents
io_slim_dtm <- removeSparseTerms(io_dtm, sparse = 0.997)
```

# **Analysis**
This chunk is to determine number of topics to extract.
```{r Topic modeling, message=FALSE}
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster)

# Use FindTopicsNumber to tune the number of topics
DTM_tune <- FindTopicsNumber(
  io_dtm,
  topics = seq(2, 10, 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  verbose = TRUE
)

stopCluster(local_cluster)
registerDoSEQ()
```

This chuck is to create topic_tbl by using beta and gamma.
```{r lda, message=FALSE}
io_lda <- LDA(io_dtm, 5)

# Create a beta matrix
lda_b <- tidy(io_lda, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta)

summary(lda_b)

# Create a gamma matrix
lda_g <- tidy(io_lda, matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document)

# Create the topic_tbl
week12_tbl_a <- week12_tbl %>%
  mutate(doc_id = as.character(1:nrow(week12_tbl)))

topic_tbl <- tibble(doc_id = Docs(io_dtm)) %>%
  left_join(y = week12_tbl_a, by = join_by(doc_id)) %>%
  mutate(topic = lda_g$topic, probability = lda_g$gamma)

# double-check to answer questions
topic_tbl
show(lda_b)
show(lda_g)
```



This chunk is to make a pretty word cloud.
```{r cloud, message=FALSE}
library(wordcloud)
DTM_tbl <- io_dtm %>% as.matrix %>% as_tibble
wordcloud(
  words = names(DTM_tbl),
  freq = colSums(DTM_tbl),
  colors = brewer.pal(9,"YlOrBr")
)
```